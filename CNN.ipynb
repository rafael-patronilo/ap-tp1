{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CustomImageDataset as CID\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torcheval.metrics import MulticlassF1Score\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "train_dataset = CID.CustomImageDataset(\n",
    "    annotations_file=\"./data/images/images/train.csv\",\n",
    "    img_dir=\"./data/images/images/train/\",\n",
    "    # transform=preprocess\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Load the test set\n",
    "val_dataset = CID.CustomImageDataset(\n",
    "    annotations_file=\"./data/images/images/test.csv\",\n",
    "    img_dir=\"./data/images/images/test/\",\n",
    "    # transform=preprocess\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    # TODO: Implement a function that creates a model with the given layer sizes\n",
    "    model = nn.Sequential(\n",
    "        nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.LazyConv2d(256, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # nn.LazyConv2d(384, kernel_size=3, padding=1),\n",
    "        # nn.ReLU(),\n",
    "        # nn.LazyConv2d(384, kernel_size=3, padding=1),\n",
    "        # nn.ReLU(),\n",
    "        # nn.LazyConv2d(256, kernel_size=3, padding=1),\n",
    "        # nn.ReLU(),\n",
    "        # nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(18),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_vgg(vgg_blocks, linear_layers):\n",
    "    layers = []\n",
    "    for (num_convs, out_channels) in vgg_blocks:\n",
    "        if num_convs > 0:\n",
    "            for _ in range(num_convs):\n",
    "                layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    layers.append(nn.Flatten())\n",
    "    for out_features in linear_layers:\n",
    "        layers.append(nn.LazyLinear(out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout())\n",
    "    layers.append(nn.LazyLinear(18))\n",
    "    return nn.Sequential(nn.LayerNorm([3, 300, 400]), *layers)\n",
    "\n",
    "def test_vgg(vgg_blocks, linear_layers):\n",
    "    model = make_vgg(vgg_blocks, linear_layers)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 15\n",
    "    model.to(device)\n",
    "    loss = None\n",
    "    accuracy = None\n",
    "    f_score = None\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch} for {tuple(vgg_blocks)} VGG blocks and {linear_layers} linear layers\")\n",
    "        loss, accuracy, f_score = train(\n",
    "            train_loader, test_loader, model, loss_fn, optimizer\n",
    "        )\n",
    "    return model, loss, accuracy, f_score\n",
    "\n",
    "def test_vgg_architectures():\n",
    "    print(\"-\" * 196)\n",
    "    out_channels = [16, 32, 64, 128, 256, 512, 512]\n",
    "    def neuron_count(architecture):\n",
    "        conv_numbers, linear_layers = architecture\n",
    "        return sum(linear_layers) + sum(out*count for out, count in zip(out_channels, conv_numbers))\n",
    "    all_conv_numbers = [x for x in itertools.product(range(4), repeat=len(out_channels)) \n",
    "                        if sum(x) >= 1 and sum(1 for y in x if y > 0) >= 2]\n",
    "    linear_layer_sizes = [32, 64, 128, 256, 1024, 4096]\n",
    "    all_linear_layers = [x for layer_size in linear_layer_sizes for x in [[layer_size], [layer_size, layer_size]]]\n",
    "    all_linear_layers.append([])\n",
    "    architectures = list(itertools.product(all_conv_numbers, all_linear_layers))\n",
    "    architectures.sort(key=neuron_count)\n",
    "    for i, architecture in enumerate(architectures[:100]):\n",
    "        conv_numbers, linear_layers = architecture\n",
    "        vgg_blocks = zip(conv_numbers, out_channels)\n",
    "        model, loss, accuracy, f_score = test_vgg(vgg_blocks, linear_layers)\n",
    "        if best_f_score is None or f_score > best_f_score:\n",
    "            best_f_score = f_score\n",
    "            print(f\"New best model found:{vgg_blocks} {linear_layers}\")\n",
    "            save_last_n(model, \"best_vgg\", 3)\n",
    "        print(f\"Best f_score so far: {best_f_score}\")\n",
    "        print(\"-\" * 196)\n",
    "        \n",
    "\n",
    "def train(train_loader, test_loader, model, loss_fn, optimizer):\n",
    "    size = len(train_loader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # y = nn.functional.one_hot(y, num_classes=18)\n",
    "        # y = torch.tensor(y.clone().detach(),dtype=torch.float32)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss, current = loss.item(), ((batch )*64+ len(X) )if not len(X)== 64 else (batch+1)*len(X)\n",
    "        # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    test_loss, accuracy, f_score = evaluate(\n",
    "        model, loss_fn, train_loader\n",
    "    )\n",
    "    print(\n",
    "        f\"Train Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f_score:>8f} \\n\"\n",
    "    )\n",
    "    test_loss, accuracy, f_score = evaluate(\n",
    "        model, loss_fn, test_loader\n",
    "    )\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f_score:>8f} \\n\"\n",
    "    )\n",
    "    return test_loss, accuracy, f_score\n",
    "\n",
    "\n",
    "def evaluate(model, loss_fn, loader):\n",
    "    total_size = len(loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        f_score = MulticlassF1Score(device=device)\n",
    "\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            f_score.update(pred, y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= len(loader)\n",
    "        correct /= total_size\n",
    "        accuracy = 100 * correct\n",
    "        f_score = f_score.compute()\n",
    "    return test_loss, accuracy, f_score\n",
    "\n",
    "\n",
    "#total_size = len(train_loader.dataset)\n",
    "# indices = list(range(total_size))\n",
    "#split = int(0.7 * total_size)\n",
    "#indices = np.arange(total_size)\n",
    "#train_indices = indices[:split]\n",
    "#test_indices = indices[split:]\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    train_loader.dataset, \n",
    "    [0.7, 0.3], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "#train_sampler = SubsetRandomSampler(train_indices)\n",
    "#test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "def save_last_n(model, name, n):\n",
    "    file = f\"{name}_{n-1}.pth\"\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "    for i in range(1, n):\n",
    "        old_file = f\"{name}_{i-1}.pth\"\n",
    "        file = f\"{name}_{i}.pth\"\n",
    "        if os.path.isfile(file):\n",
    "            os.rename(old_file, file)\n",
    "    torch.save(model, f\"{name}_0.pth\")\n",
    "\n",
    "\n",
    "def test_architecture():\n",
    "    model = make_model()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 15\n",
    "    model.to(device)\n",
    "    loss = None\n",
    "    accuracy = None\n",
    "    f_score = None\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        loss, accuracy, f_score = train(\n",
    "            train_loader, test_loader, model, loss_fn, optimizer\n",
    "        )\n",
    "    return model, loss, accuracy, f_score\n",
    "\n",
    "\n",
    "def train_indefinitely(model):\n",
    "    epoch = 0\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_f_score = None\n",
    "    try:\n",
    "        while True:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            loss, accuracy, f_score = train(\n",
    "                train_loader, test_loader, model, loss_fn, optimizer\n",
    "            )\n",
    "            if epoch % 25 == 0:\n",
    "                print(\"Saving model\")\n",
    "                save_last_n(model, \"training_mlp\", 3)\n",
    "                if best_f_score is None or f_score > best_f_score:\n",
    "                    best_f_score = f_score\n",
    "                    print(\"New best model found\")\n",
    "                    save_last_n(model, \"training_best_mlp\", 1)\n",
    "            epoch += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training stopped, saving current model\")\n",
    "        save_last_n(model, \"training_mlp\", 4)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Saving current model\")\n",
    "        save_last_n(model, \"training_mlp\", 4)\n",
    "\n",
    "def create_submission(model):\n",
    "    #model = torch.load(\"training_mlp_0.pth\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # pred = torch.tensor([])\n",
    "    pred = np.array([])\n",
    "\n",
    "    for X, Y in val_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        pred = np.concatenate((pred, model(X).cpu().detach().numpy().argmax(axis=1)))\n",
    "    # pred = pred.cpu().detach().numpy().argmax(axis=1)\n",
    "    print(pred)\n",
    "    submission = pd.DataFrame({\"Id\": val_dataset.img_labels.iloc[:, 0], \"main_type\": pred})\n",
    "    submission.to_csv(\"./submission_last.csv\", index=False)\n",
    "\n",
    "# test_mlp_architectures()\n",
    "# train_indefinitely(torch.load(\"best_mlp_0.pth\"))\n",
    "test_vgg_architectures()\n",
    "\n",
    "#create_submission(torch.load(\"best_mlp_0.pth\"))\n",
    "#create_submission(torch.load(\"training_mlp_0.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
