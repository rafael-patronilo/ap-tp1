{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CustomImageDataset as CID\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import os\n",
    "from torcheval.metrics import MulticlassF1Score\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "train_dataset = CID.CustomImageDataset(\n",
    "    annotations_file=\"./data/images/images/train.csv\",\n",
    "    img_dir=\"./data/images/images/train/\",\n",
    "    # transform=preprocess\n",
    ")\n",
    "\n",
    "# Load the test set\n",
    "val_dataset = CID.CustomImageDataset(\n",
    "    annotations_file=\"./data/images/images/test.csv\",\n",
    "    img_dir=\"./data/images/images/test/\",\n",
    "    # transform=preprocess\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, \n",
    "    [0.7, 0.3], \n",
    "    generator=torch.Generator().manual_seed(57473)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "def prepare_pretrained_model(model):\n",
    "    model.fc = nn.Linear(model.fc.in_features, 18)\n",
    "    nn.init.xavier_uniform_(model.fc.weight)\n",
    "\n",
    "def train(train_loader, test_loader, model, loss_fn, optimizer):\n",
    "    size = len(train_loader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        print(\".\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # y = nn.functional.one_hot(y, num_classes=18)\n",
    "        # y = torch.tensor(y.clone().detach(),dtype=torch.float32)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss, current = loss.item(), ((batch )*64+ len(X) )if not len(X)== 64 else (batch+1)*len(X)\n",
    "        # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    test_loss, accuracy, f_score = evaluate(\n",
    "        model, loss_fn, train_loader\n",
    "    )\n",
    "    print(\n",
    "        f\"Train Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f_score:>8f} \\n\"\n",
    "    )\n",
    "    test_loss, accuracy, f_score = evaluate(\n",
    "        model, loss_fn, test_loader\n",
    "    )\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f_score:>8f} \\n\"\n",
    "    )\n",
    "    return test_loss, accuracy, f_score\n",
    "\n",
    "def save_last_n(model, name, n):\n",
    "    file = f\"{name}_{n-1}.pth\"\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "    for i in range(1, n):\n",
    "        old_file = f\"{name}_{i-1}.pth\"\n",
    "        file = f\"{name}_{i}.pth\"\n",
    "        if os.path.isfile(file):\n",
    "            os.rename(old_file, file)\n",
    "    torch.save(model, f\"{name}_0.pth\")\n",
    "\n",
    "def evaluate(model, loss_fn, loader):\n",
    "    total_size = len(loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        f_score = MulticlassF1Score(device=device)\n",
    "\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            f_score.update(pred, y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= len(loader)\n",
    "        correct /= total_size\n",
    "        accuracy = 100 * correct\n",
    "        f_score = f_score.compute()\n",
    "    return test_loss, accuracy, f_score\n",
    "\n",
    "def train_fine_tuning(model, learning_rate,\n",
    "                      param_group=True):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = None\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in model.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        optimizer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': model.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    epoch = 0\n",
    "    best_f_score = None\n",
    "    try:\n",
    "        while True:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            loss, accuracy, f_score = train(\n",
    "                train_loader, test_loader, model, loss_fn, optimizer\n",
    "            )\n",
    "            if epoch % 25 == 0:\n",
    "                print(\"Saving model\")\n",
    "                save_last_n(model, \"training_finetune\", 3)\n",
    "                if best_f_score is None or f_score > best_f_score:\n",
    "                    best_f_score = f_score\n",
    "                    print(\"New best model found\")\n",
    "                    save_last_n(model, \"training_best_finetune\", 1)\n",
    "            epoch += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training stopped, saving current model\")\n",
    "        save_last_n(model, \"training_finetune\", 4)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        save_last_n(model, \"training_finetune\", 4)\n",
    "        raise e\n",
    "        \n",
    "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
    "prepare_pretrained_model(finetune_net)\n",
    "train_fine_tuning(finetune_net, 0.001, param_group=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
